package main

import (
	"fmt"
	"log"
	"math"
	"strings"

	"github.com/blackmoon87/thinkingnet/pkg/algorithms"
	"github.com/blackmoon87/thinkingnet/pkg/core"
	"github.com/blackmoon87/thinkingnet/pkg/preprocessing"
)

func main() {
	fmt.Println("=== ThinkingNet-Go Easy Regression Demo ===")
	fmt.Println("Ù…Ø«Ø§Ù„ Ø´Ø§Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ThinkingNet-Go")
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù†Ø­Ø¯Ø§Ø± ØªØ¬Ø±ÙŠØ¨ÙŠØ©
	// Step 1: Create synthetic regression data
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù†Ø­Ø¯Ø§Ø± ØªØ¬Ø±ÙŠØ¨ÙŠØ©")
	fmt.Println("Step 1: Creating synthetic regression data")

	X, y := createRegressionData(200, 2, 0.1) // 200 samples, 2 features, noise level 0.1

	fmt.Printf("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: %d Ø¹ÙŠÙ†Ø©ØŒ %d Ø®Ø§ØµÙŠØ©\n", X.Shape()[0], X.Shape()[1])
	fmt.Printf("Data created: %d samples, %d features\n", X.Shape()[0], X.Shape()[1])
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
	// Step 2: Data exploration
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
	fmt.Println("Step 2: Data exploration")

	exploreRegressionData(X, y)
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±
	// Step 3: Split data into training and testing sets
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (80% ØªØ¯Ø±ÙŠØ¨ØŒ 20% Ø§Ø®ØªØ¨Ø§Ø±)")
	fmt.Println("Step 3: Split data (80% training, 20% testing)")

	XTrain, XTest, yTrain, yTest, err := preprocessing.EasySplit(X, y, 0.2)
	if err != nil {
		log.Fatalf("Ø®Ø·Ø£ ÙÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Error splitting data: %v", err)
	}

	fmt.Printf("Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: %d Ø¹ÙŠÙ†Ø©\n", XTrain.Shape()[0])
	fmt.Printf("Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: %d Ø¹ÙŠÙ†Ø©\n", XTest.Shape()[0])
	fmt.Printf("Training data: %d samples\n", XTrain.Shape()[0])
	fmt.Printf("Testing data: %d samples\n", XTest.Shape()[0])
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 4: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø§Ù„ØªØ·Ø¨ÙŠØ¹)
	// Step 4: Data preprocessing (normalization)
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… StandardScaler")
	fmt.Println("Step 4: Normalize data using StandardScaler")

	XTrainScaled, err := preprocessing.EasyStandardScale(XTrain)
	if err != nil {
		log.Fatalf("Ø®Ø·Ø£ ÙÙŠ ØªØ·Ø¨ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ / Error scaling training data: %v", err)
	}

	XTestScaled, err := preprocessing.EasyStandardScale(XTest)
	if err != nil {
		log.Fatalf("Ø®Ø·Ø£ ÙÙŠ ØªØ·Ø¨ÙŠØ¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± / Error scaling test data: %v", err)
	}

	fmt.Println("ØªÙ… ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
	fmt.Println("Data normalized successfully")
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 5: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·
	// Step 5: Train simple Linear Regression model
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 5: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø§Ù„Ø¨Ø³ÙŠØ·")
	fmt.Println("Step 5: Training simple Linear Regression model")

	// Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
	// Use helper function to create the model
	linearModel := algorithms.EasyLinearRegression()

	err = linearModel.Fit(XTrainScaled, yTrain)
	if err != nil {
		log.Fatalf("Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Error training model: %v", err)
	}

	fmt.Println("ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Ø¨Ù†Ø¬Ø§Ø­")
	fmt.Println("Linear Regression model trained successfully")
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 6: ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø³ÙŠØ·
	// Step 6: Evaluate simple model
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 6: ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø³ÙŠØ·")
	fmt.Println("Step 6: Simple model evaluation")

	evaluateRegressionModel(linearModel, XTrainScaled, yTrain, XTestScaled, yTest, "Linear Regression (Simple)")
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 7: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù†Ø­Ø¯Ø§Ø± Ø®Ø·ÙŠ Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ… L2
	// Step 7: Train Linear Regression with L2 regularization
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 7: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù†Ø­Ø¯Ø§Ø± Ø®Ø·ÙŠ Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ… L2")
	fmt.Println("Step 7: Training Linear Regression with L2 regularization")

	// Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ… L2
	// Create model with L2 regularization
	l2Model := algorithms.NewLinearRegression(
		algorithms.WithLinearLearningRate(0.01),
		algorithms.WithLinearMaxIterations(1000),
		algorithms.WithLinearTolerance(1e-6),
		algorithms.WithLinearRegularization("l2", 0.1),
		algorithms.WithLinearFitIntercept(true),
	)

	err = l2Model.Fit(XTrainScaled, yTrain)
	if err != nil {
		log.Fatalf("Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ L2 / Error training L2 model: %v", err)
	}

	fmt.Println("ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ… L2 Ø¨Ù†Ø¬Ø§Ø­")
	fmt.Println("L2 regularized Linear Regression model trained successfully")
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 8: ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ L2
	// Step 8: Evaluate L2 model
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 8: ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ L2")
	fmt.Println("Step 8: L2 model evaluation")

	evaluateRegressionModel(l2Model, XTrainScaled, yTrain, XTestScaled, yTest, "Linear Regression (L2)")
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 9: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù†Ø­Ø¯Ø§Ø± Ø®Ø·ÙŠ Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ… Elastic Net
	// Step 9: Train Linear Regression with Elastic Net regularization
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 9: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù†Ø­Ø¯Ø§Ø± Ø®Ø·ÙŠ Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ… Elastic Net")
	fmt.Println("Step 9: Training Linear Regression with Elastic Net regularization")

	// Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ… Elastic Net
	// Create model with Elastic Net regularization
	elasticModel := algorithms.NewLinearRegression(
		algorithms.WithLinearLearningRate(0.01),
		algorithms.WithLinearMaxIterations(1000),
		algorithms.WithLinearTolerance(1e-6),
		algorithms.WithLinearElasticNet(0.1, 0.5), // lambda=0.1, l1_ratio=0.5
		algorithms.WithLinearFitIntercept(true),
	)

	err = elasticModel.Fit(XTrainScaled, yTrain)
	if err != nil {
		log.Fatalf("Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Elastic Net / Error training Elastic Net model: %v", err)
	}

	fmt.Println("ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ… Elastic Net Ø¨Ù†Ø¬Ø§Ø­")
	fmt.Println("Elastic Net Linear Regression model trained successfully")
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 10: ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ Elastic Net
	// Step 10: Evaluate Elastic Net model
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 10: ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Elastic Net")
	fmt.Println("Step 10: Elastic Net model evaluation")

	evaluateRegressionModel(elasticModel, XTrainScaled, yTrain, XTestScaled, yTest, "Linear Regression (Elastic Net)")
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 11: Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ¹Ø±Ø¶ Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
	// Step 11: Compare models and show prediction examples
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 11: Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª")
	fmt.Println("Step 11: Model comparison and prediction examples")

	compareRegressionModels(linearModel, l2Model, elasticModel, XTestScaled, yTest)
	fmt.Println()

	// Ø§Ù„Ø®Ø·ÙˆØ© 12: Ù†ØµØ§Ø¦Ø­ Ù„Ù„ØªØ­Ø³ÙŠÙ† ÙˆØ§Ù„ØªØµÙˆØ±
	// Step 12: Tips for improvement and visualization
	fmt.Println("Ø§Ù„Ø®Ø·ÙˆØ© 12: Ù†ØµØ§Ø¦Ø­ Ù„Ù„ØªØ­Ø³ÙŠÙ† ÙˆØ§Ù„ØªØµÙˆØ±")
	fmt.Println("Step 12: Tips for improvement and visualization")

	showImprovementTips()
	fmt.Println()

	fmt.Println("=== Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­! ===")
	fmt.Println("=== Demo completed successfully! ===")
}

// createRegressionData ÙŠÙ†Ø´Ø¦ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù†Ø­Ø¯Ø§Ø± ØªØ¬Ø±ÙŠØ¨ÙŠØ©
// createRegressionData creates synthetic regression data
func createRegressionData(nSamples, nFeatures int, noiseLevel float64) (core.Tensor, core.Tensor) {
	// Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
	// Create random feature data
	XData := make([][]float64, nSamples)
	yData := make([][]float64, nSamples)

	// Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„Ù„Ø§Ù†Ø­Ø¯Ø§Ø±
	// True regression coefficients
	trueCoeffs := []float64{2.5, -1.8} // Ù„Ù„Ù…ÙŠØ²ØªÙŠÙ†
	intercept := 1.2

	for i := 0; i < nSamples; i++ {
		features := make([]float64, nFeatures)

		// Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
		// Generate random features
		for j := 0; j < nFeatures; j++ {
			features[j] = (float64(i%100)/50.0 - 1.0) + 0.5*math.Sin(float64(i)*0.1) // Ù†Ù…Ø· Ù…Ù†ØªØ¸Ù… Ù…Ø¹ ØªÙ†ÙˆØ¹
		}

		// Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©
		// Calculate target value
		target := intercept
		for j := 0; j < nFeatures; j++ {
			target += trueCoeffs[j] * features[j]
		}

		// Ø¥Ø¶Ø§ÙØ© Ø¶ÙˆØ¶Ø§Ø¡
		// Add noise
		noise := (math.Sin(float64(i)*0.05) + math.Cos(float64(i)*0.03)) * noiseLevel
		target += noise

		XData[i] = features
		yData[i] = []float64{target}
	}

	X := core.NewTensorFromSlice(XData)
	y := core.NewTensorFromSlice(yData)

	return X, y
}

// exploreRegressionData ÙŠØ³ØªÙƒØ´Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± ÙˆÙŠØ¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø£Ø³Ø§Ø³ÙŠØ©
// exploreRegressionData explores regression data and shows basic statistics
func exploreRegressionData(X, y core.Tensor) {
	rows, cols := X.Dims()

	fmt.Printf("Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: (%d, %d)\n", rows, cols)
	fmt.Printf("Data shape: (%d, %d)\n", rows, cols)

	// Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø®ØµØ§Ø¦Øµ
	// Calculate feature statistics
	fmt.Println("Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø®ØµØ§Ø¦Øµ / Feature statistics:")
	for j := 0; j < cols; j++ {
		var sum, min, max float64
		min = X.At(0, j)
		max = X.At(0, j)

		for i := 0; i < rows; i++ {
			val := X.At(i, j)
			sum += val
			if val < min {
				min = val
			}
			if val > max {
				max = val
			}
		}

		mean := sum / float64(rows)
		fmt.Printf("  Ø§Ù„Ø®Ø§ØµÙŠØ© %d / Feature %d: Ù…ØªÙˆØ³Ø·/Mean=%.3f, Ø£Ø¯Ù†Ù‰/Min=%.3f, Ø£Ø¹Ù„Ù‰/Max=%.3f\n",
			j+1, j+1, mean, min, max)
	}

	// Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
	// Calculate target variable statistics
	var ySum, yMin, yMax float64
	yRows, _ := y.Dims()
	yMin = y.At(0, 0)
	yMax = y.At(0, 0)

	for i := 0; i < yRows; i++ {
		val := y.At(i, 0)
		ySum += val
		if val < yMin {
			yMin = val
		}
		if val > yMax {
			yMax = val
		}
	}

	yMean := ySum / float64(yRows)
	fmt.Printf("Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù / Target variable: Ù…ØªÙˆØ³Ø·/Mean=%.3f, Ø£Ø¯Ù†Ù‰/Min=%.3f, Ø£Ø¹Ù„Ù‰/Max=%.3f\n",
		yMean, yMin, yMax)

	// Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù„Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
	// Calculate standard deviation of target variable
	var yVariance float64
	for i := 0; i < yRows; i++ {
		diff := y.At(i, 0) - yMean
		yVariance += diff * diff
	}
	yStd := math.Sqrt(yVariance / float64(yRows))
	fmt.Printf("Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù„Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù / Target std: %.3f\n", yStd)
}

// evaluateRegressionModel ÙŠÙ‚ÙŠÙ… Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± ÙˆÙŠØ¹Ø±Ø¶ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
// evaluateRegressionModel evaluates regression model performance and shows metrics
func evaluateRegressionModel(model *algorithms.LinearRegression, XTrain, yTrain, XTest, yTest core.Tensor, modelName string) {
	fmt.Printf("ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ %s / Evaluating %s model:\n", modelName, modelName)

	// ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
	// Evaluate on training data
	trainR2, err := model.Score(XTrain, yTrain)
	if err != nil {
		log.Printf("Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ RÂ² Ù„Ù„ØªØ¯Ø±ÙŠØ¨ / Error calculating training RÂ²: %v", err)
		return
	}

	// ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
	// Evaluate on test data
	testR2, err := model.Score(XTest, yTest)
	if err != nil {
		log.Printf("Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ RÂ² Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± / Error calculating test RÂ²: %v", err)
		return
	}

	// Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
	// Get predictions for detailed metrics
	trainPredictions, err := model.Predict(XTrain)
	if err != nil {
		log.Printf("Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ / Error making training predictions: %v", err)
		return
	}

	testPredictions, err := model.Predict(XTest)
	if err != nil {
		log.Printf("Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± / Error making test predictions: %v", err)
		return
	}

	// Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
	// Calculate detailed metrics
	trainMetrics := algorithms.CalculateRegressionMetrics(yTrain, trainPredictions)
	testMetrics := algorithms.CalculateRegressionMetrics(yTest, testPredictions)

	fmt.Printf("  Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ / Training Performance:\n")
	fmt.Printf("    RÂ² Score: %.4f\n", trainR2)
	fmt.Printf("    MSE: %.4f\n", trainMetrics.MSE)
	fmt.Printf("    RMSE: %.4f\n", trainMetrics.RMSE)
	fmt.Printf("    MAE: %.4f\n", trainMetrics.MAE)

	fmt.Printf("  Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± / Test Performance:\n")
	fmt.Printf("    RÂ² Score: %.4f\n", testR2)
	fmt.Printf("    MSE: %.4f\n", testMetrics.MSE)
	fmt.Printf("    RMSE: %.4f\n", testMetrics.RMSE)
	fmt.Printf("    MAE: %.4f\n", testMetrics.MAE)

	// Ø¹Ø±Ø¶ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
	// Show model coefficients
	weights := model.Weights()
	if weights != nil {
		fmt.Printf("  Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Model Coefficients:\n")
		weightsRows, _ := weights.Dims()
		fmt.Printf("    Ø§Ù„Ù…Ù‚Ø·Ø¹ / Intercept: %.4f\n", weights.At(0, 0))
		for i := 1; i < weightsRows; i++ {
			fmt.Printf("    Ø§Ù„Ù…ÙŠØ²Ø© %d / Feature %d: %.4f\n", i, i, weights.At(i, 0))
		}
	}

	// ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
	// Analyze overfitting
	overfit := trainR2 - testR2
	if overfit > 0.1 {
		fmt.Printf("  âš ï¸  ØªØ­Ø°ÙŠØ±: Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (ÙØ±Ù‚ RÂ²: %.4f)\n", overfit)
		fmt.Printf("  âš ï¸  Warning: Possible overfitting (RÂ² difference: %.4f)\n", overfit)
	} else if overfit < -0.05 {
		fmt.Printf("  â„¹ï¸  Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (ÙØ±Ù‚ RÂ²: %.4f)\n", overfit)
		fmt.Printf("  â„¹ï¸  Note: Model might need more training (RÂ² difference: %.4f)\n", overfit)
	} else {
		fmt.Printf("  âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ØªÙˆØ§Ø²Ù† Ø¬ÙŠØ¯Ø§Ù‹ (ÙØ±Ù‚ RÂ²: %.4f)\n", overfit)
		fmt.Printf("  âœ… Model is well balanced (RÂ² difference: %.4f)\n", overfit)
	}
}

// compareRegressionModels ÙŠÙ‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± ÙˆÙŠØ¹Ø±Ø¶ Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
// compareRegressionModels compares regression models and shows prediction examples
func compareRegressionModels(simpleModel, l2Model, elasticModel *algorithms.LinearRegression, XTest, yTest core.Tensor) {
	fmt.Println("Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
	fmt.Println("Model comparison on test data samples:")

	// Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
	// Get predictions from all models
	simplePreds, err := simpleModel.Predict(XTest)
	if err != nil {
		log.Printf("Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø³ÙŠØ· / Error in simple model predictions: %v", err)
		return
	}

	l2Preds, err := l2Model.Predict(XTest)
	if err != nil {
		log.Printf("Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¨Ø¤Ø§Øª Ù†Ù…ÙˆØ°Ø¬ L2 / Error in L2 model predictions: %v", err)
		return
	}

	elasticPreds, err := elasticModel.Predict(XTest)
	if err != nil {
		log.Printf("Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¨Ø¤Ø§Øª Ù†Ù…ÙˆØ°Ø¬ Elastic Net / Error in Elastic Net model predictions: %v", err)
		return
	}

	fmt.Println()
	fmt.Printf("%-10s %-10s %-12s %-12s %-12s %-12s %-12s %-12s\n",
		"Feature1", "Feature2", "True", "Simple", "L2", "Elastic", "Simple_Err", "L2_Err")
	fmt.Printf("%-10s %-10s %-12s %-12s %-12s %-12s %-12s %-12s\n",
		"Ø§Ù„Ø®Ø§ØµÙŠØ©1", "Ø§Ù„Ø®Ø§ØµÙŠØ©2", "Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ", "Ø§Ù„Ø¨Ø³ÙŠØ·", "L2", "Elastic", "Ø®Ø·Ø£_Ø§Ù„Ø¨Ø³ÙŠØ·", "Ø®Ø·Ø£_L2")
	fmt.Println(strings.Repeat("-", 100))

	// Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 10 Ø¹ÙŠÙ†Ø§Øª
	// Show first 10 samples
	rows, _ := XTest.Dims()
	maxSamples := 10
	if rows < maxSamples {
		maxSamples = rows
	}

	for i := 0; i < maxSamples; i++ {
		feature1 := XTest.At(i, 0)
		feature2 := XTest.At(i, 1)
		trueVal := yTest.At(i, 0)
		simplePred := simplePreds.At(i, 0)
		l2Pred := l2Preds.At(i, 0)
		elasticPred := elasticPreds.At(i, 0)

		simpleErr := math.Abs(trueVal - simplePred)
		l2Err := math.Abs(trueVal - l2Pred)

		fmt.Printf("%-10.3f %-10.3f %-12.3f %-12.3f %-12.3f %-12.3f %-12.3f %-12.3f\n",
			feature1, feature2, trueVal, simplePred, l2Pred, elasticPred, simpleErr, l2Err)
	}

	// Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
	// Calculate overall metrics for comparison
	fmt.Println("\nÙ…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© / Overall Metrics Comparison:")
	fmt.Println(strings.Repeat("-", 60))

	simpleMetrics := algorithms.CalculateRegressionMetrics(yTest, simplePreds)
	l2Metrics := algorithms.CalculateRegressionMetrics(yTest, l2Preds)
	elasticMetrics := algorithms.CalculateRegressionMetrics(yTest, elasticPreds)

	fmt.Printf("%-20s %-12s %-12s %-12s\n", "Metric", "Simple", "L2", "Elastic")
	fmt.Printf("%-20s %-12s %-12s %-12s\n", "Ø§Ù„Ù…Ù‚ÙŠØ§Ø³", "Ø§Ù„Ø¨Ø³ÙŠØ·", "L2", "Elastic")
	fmt.Println(strings.Repeat("-", 60))
	fmt.Printf("%-20s %-12.4f %-12.4f %-12.4f\n", "RÂ² Score", simpleMetrics.R2Score, l2Metrics.R2Score, elasticMetrics.R2Score)
	fmt.Printf("%-20s %-12.4f %-12.4f %-12.4f\n", "MSE", simpleMetrics.MSE, l2Metrics.MSE, elasticMetrics.MSE)
	fmt.Printf("%-20s %-12.4f %-12.4f %-12.4f\n", "RMSE", simpleMetrics.RMSE, l2Metrics.RMSE, elasticMetrics.RMSE)
	fmt.Printf("%-20s %-12.4f %-12.4f %-12.4f\n", "MAE", simpleMetrics.MAE, l2Metrics.MAE, elasticMetrics.MAE)

	// ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
	// Determine best model
	bestR2 := simpleMetrics.R2Score
	bestModel := "Simple"

	if l2Metrics.R2Score > bestR2 {
		bestR2 = l2Metrics.R2Score
		bestModel = "L2"
	}

	if elasticMetrics.R2Score > bestR2 {
		bestR2 = elasticMetrics.R2Score
		bestModel = "Elastic Net"
	}

	fmt.Printf("\nğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ / Best Model: %s (RÂ² = %.4f)\n", bestModel, bestR2)
}

// showImprovementTips ÙŠØ¹Ø±Ø¶ Ù†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ØªØµÙˆØ±
// showImprovementTips shows tips for performance improvement and visualization
func showImprovementTips() {
	fmt.Println("Ù†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±:")
	fmt.Println("Tips for improving regression models:")
	fmt.Println()

	fmt.Println("1. ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Data Improvement:")
	fmt.Println("   - Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Collect more data")
	fmt.Println("   - Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø°Ø§Øª ØµÙ„Ø© / Add relevant new features")
	fmt.Println("   - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© / Handle missing values")
	fmt.Println("   - Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© / Remove outliers")
	fmt.Println()

	fmt.Println("2. Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª / Feature Engineering:")
	fmt.Println("   - Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø§Øª ØªÙØ§Ø¹Ù„ÙŠØ© / Create interaction features")
	fmt.Println("   - ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª (log, sqrt, polynomial) / Transform features")
	fmt.Println("   - ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª / Normalize features")
	fmt.Println("   - Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© / Select important features")
	fmt.Println()

	fmt.Println("3. Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Model Tuning:")
	fmt.Println("   - ØªØ¬Ø±Ø¨Ø© Ù‚ÙŠÙ… Ù…Ø®ØªÙ„ÙØ© Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ¹Ù„Ù… / Try different learning rates")
	fmt.Println("   - Ø¶Ø¨Ø· Ù‚ÙˆØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ… / Tune regularization strength")
	fmt.Println("   - Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª / Increase iterations")
	fmt.Println("   - ØªØ¬Ø±Ø¨Ø© Ø£Ù†ÙˆØ§Ø¹ ØªÙ†Ø¸ÙŠÙ… Ù…Ø®ØªÙ„ÙØ© / Try different regularization types")
	fmt.Println()

	fmt.Println("4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Model Validation:")
	fmt.Println("   - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ / Use cross-validation")
	fmt.Println("   - ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨/ØªØ­Ù‚Ù‚/Ø§Ø®ØªØ¨Ø§Ø± / Split into train/val/test")
	fmt.Println("   - Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ù†Ø­Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… / Monitor learning curves")
	fmt.Println("   - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ / Analyze residuals")
	fmt.Println()

	fmt.Println("5. Ø§Ù„ØªØµÙˆØ± ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ / Visualization & Analysis:")
	fmt.Println("   - Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© / Plot original data")
	fmt.Println("   - Ø±Ø³Ù… Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© / Plot predictions vs actual")
	fmt.Println("   - Ø±Ø³Ù… Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ / Plot residuals")
	fmt.Println("   - ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ / Analyze error distribution")
	fmt.Println()

	fmt.Println("6. Ù†Ù…Ø§Ø°Ø¬ Ù…ØªÙ‚Ø¯Ù…Ø© / Advanced Models:")
	fmt.Println("   - ØªØ¬Ø±Ø¨Ø© Random Forest Ù„Ù„Ø§Ù†Ø­Ø¯Ø§Ø± / Try Random Forest regression")
	fmt.Println("   - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© / Use neural networks")
	fmt.Println("   - ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ / Try ensemble models")
	fmt.Println("   - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ ØºÙŠØ± Ø®Ø·ÙŠØ© / Use non-linear models")
	fmt.Println()

	fmt.Println("Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ÙƒÙˆØ¯ Ø§Ù„ØªØµÙˆØ± (ÙŠØªØ·Ù„Ø¨ Ù…ÙƒØªØ¨Ø© Ø±Ø³Ù…):")
	fmt.Println("Example visualization code (requires plotting library):")
	fmt.Println(`
// Ø±Ø³Ù… Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
// Plot predictions vs actual values
func plotPredictionsVsActual(yTrue, yPred core.Tensor) {
    // Ø§Ø³ØªØ®Ø¯Ù… Ù…ÙƒØªØ¨Ø© Ù…Ø«Ù„ gonum/plot Ø£Ùˆ go-echarts
    // Use a library like gonum/plot or go-echarts
    
    // Ø¥Ù†Ø´Ø§Ø¡ scatter plot
    // Create scatter plot
    // x-axis: Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© / True values
    // y-axis: Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª / Predictions
    // Ø®Ø· Ù…Ø«Ø§Ù„ÙŠ: y = x / Perfect line: y = x
}

// Ø±Ø³Ù… Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ
// Plot residuals
func plotResiduals(yTrue, yPred core.Tensor) {
    // Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ / Calculate residuals
    // residuals = yTrue - yPred
    
    // Ø±Ø³Ù… Ø§Ù„Ø¨ÙˆØ§Ù‚ÙŠ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
    // Plot residuals vs predictions
    // ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø­ÙˆÙ„ Ø§Ù„ØµÙØ±
    // Should be random around zero
}`)
}
